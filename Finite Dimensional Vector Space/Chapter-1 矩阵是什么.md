在线性代数的学习中，矩阵是一个重要概念。然而，解释矩阵到底是什么是一件困难的事：
因为它有时是方程组的系数
$$
\begin{cases}
a_{11} x_1+a_{12} x_2=b_1\\
a_{21} x_1+a_{22} x_2=b_2
\end{cases}
\Longrightarrow
\begin{bmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}
\end{bmatrix}
$$

有时是向量组
$$
A=(\alpha_1,\alpha_2,\alpha_3)
$$

有时是线性映射
$$
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}=
\begin{bmatrix}
a x+b y\\
c x+d y
\end{bmatrix}
$$

还与行列式有关
$$
\begin{vmatrix}
a & b\\
c & d
\end{vmatrix}
$$

要深刻地理解矩阵是什么，就要从线性代数的根基——向量空间说起。

# Section 1: 向量空间

在给出明确的定义前，我们先看一个例子：平面向量。

一个平面向量是由原点指向另一个点的有向线段，两个向量相加有平行四边形法则，一个数乘以向量表现为向量的伸缩，并且这些运算具有我们希望能够成立的所有性质（交换性，结合性，分配性等等）。

而另一边，我们也称有两个数的数组$(x,y)$是向量，它也有自己的加法和数乘：
$$
(x_1,y_1)+(x_2,y_2)=(x_1+x_2,y_1+y_2),\lambda(x,y)=(\lambda x,\lambda y)
$$
并且这些运算同样也满足交换性，结合性等性质。

从一个角度看，这两者几乎是等同的，以至于在大多数情况下，我们不会区分它们，而是称左边是右边的“几何意义”，右边是左边的“坐标表示”。

但从另一个角度看，这两者又完全不一样：你看，左边只是一些特定的几何图形，而右边则是两个数字构成的有序组，图形的变化与数字的加减乘除也完全没有共同之处。

那么，哪个观点是对的呢？这就涉及到现代数学的一个重要原则：

> 强调形式而非意义，重视关联而非对象。

在这里，“意义”是向量这个词的具体所指（有向线段或者有序组），而“形式”则是加法与数乘满足的性质。也就是说，我们不关心向量究竟是什么，我们只关心如何对向量进行运算。“向量”一词的含义应当像几何中的“点”，“线”，“面”一样作为一个未定义项，由约束它的性质（公理）隐含地确定。根据这个原则，我们给出向量空间的定义。

---

**定义 1.1.1** 向量空间（Vector Space）

四元组$(V,\mathbb F,+,\cdot)$是一个**向量空间**如果对任意的$u,v,w\in V$和$a,b\in \mathbb F$有：
- 封闭性：$v+w\in V,av\in V$
- 交换性：$v+w=w+v$
- 结合性：$(u+v)+w=u+(v+w),(ab)v=a(bv)$
- 单位元：存在$0\in V,1\in \mathbb F$使得$v+0=v,1v=v$
- 加法逆元：存在$-v\in V$使得$v+(-v)=0$
- 分配性：$a(v+w)=av+aw,(a+b)v=av+bv$

这里的$\mathbb F$可以是实数集或者复数集，以便于我们能够同时研究两者。如果数域$\mathbb F$以及加法和数乘是明确的，就可以直接记为向量空间$V$。

# Section 2: 基与维数

还记得平面向量的坐标表示吗？实际上坐标表示的本质是我们把一个向量写成$x$轴和$y$轴方向的单位向量的线性组合：$v=xi+yj$。我们称$i,j$是一组基向量。现在我们想把基的概念推广到一般的线性空间中去，那么我们就要准确把握基的“形式”，也就是存在性（每个向量都可分解）与唯一性（向量的坐标是唯一的），写成数学语言，就是**张成**与**线性无关**。

---

**定义 1.2.1** 张成（Span）

向量组$v_1,\dots,v_n$的**张成**定义为
$$\mathrm{span}(v_1,\dots,v_n)=\{a_1v_1+\dots+a_nv_n\colon a_j\in \mathbb F\}$$

空组的张成定义为$\mathrm{span}(\varnothing)=\{0\}$。这样定义是为了防止一些定理出现平凡的例外。

如果$V=\mathrm{span}(v_1,\dots,v_n)$，那么就称$v_1,\dots,v_n$是$V$的一个张成组。

---

**定义 1.2.2** 线性无关（Linear Independence）

向量组$v_1,\dots,v_n$**线性无关**如果
$$a_1v_1+\dots+a_nv_n=0\text{当且仅当}a_1=\dots=a_n=0$$

向量组**线性相关**如果它不是线性无关的。

---

现在，基的定义就是显然的了：如果向量组$v_1,\dots,v_n$张成$V$且线性无关，那么$(v_1,\dots,v_n)$就是$V$的一个**基**。

我们前面说过，重视关联多于对象本身，下面的定理建立起了张成与线性无关之间的关联：

---

**定理 1.2.3** 线性相关性引理

如果$v_1,\dots,v_n$线性相关，则有以下结论成立：
1. 存在$v_j$使得$v_j\in\mathrm{span}(v_1,\dots,v_{j-1})$
2. 去除$v_j$不改变张成的空间$\mathrm{span}(v_1,\dots,v_n)$

**证明** 

$v_1,\dots,v_n$线性相关，那么存在不全为零的数$a_1,\dots,a_n$使得$a_1v_1+\dots+a_nv_n=0$，设$a_j$是非零数中的最后一个，那么有
$$v_j=-\frac{a_1}{a_j}v_1-\dots-\frac{a_{j-1}}{a_j}v_{j-1}$$
于是就证明了1，在$a_1v_1+\dots+a_nv_n$中把上式代入，就证明了2。

---

有了这个引理，我们可以证明很多结果，例如：
- 线性无关组的长度 $\le$ 张成组的长度（从而所有基的长度相等）
- $V$的张成组可以化简成$V$的一个基
- $V$的线性无关组可以扩充成$V$的一个基

通过反复使用线性相关性引理，读者不难证明上述命题。

既然基的长度不随基的选取而改变，我们就把这个长度定义为向量空间的**维数**，记作$\dim V$。现在，我们就可以说直线是一维的，平面是二维的，空间是三维的，等等。

# Section 3: 线性映射

现在我们终于准备好去回答“矩阵是什么”的问题了。这里我们先给出结论，然后再解释它的含义：

> 矩阵是**线性映射**在**基**上的**表示**。

首先我们来解释线性映射是什么，顾名思义，如果一个映射是线性的，那么就是一个线性映射，具体地说，就是：

---

**定义 1.3.1** 线性映射（Linear Map）

设$V,W$都是线性空间，一个映射$T\colon V\to W$是**线性的**如果对任意$u,v\in V$和$\lambda\in \mathbb F$有：
- 加性：$T(u+v)=Tu+Tv$
- 齐次性：$T(\lambda v)=\lambda Tv$

---

直观地看，一个线性映射保持向量之间的线性关系不变：
$$T(xi+yj)=xT(i)+yT(j)$$

变换后的向量是变换后的基向量的线性组合，并且系数$x,y$不变。这表明，***一个线性映射完全由它对基向量的作用决定***，即
$$T(a_1v_1+\dots+a_nv_n)=a_1Tv_1+\dots+a_nTv_n$$

这个结果非常重要，它是我们构造矩阵的关键。

现在，让我们简化一些标记，我们再次从平面向量的坐标表示中获得灵感，对于向量空间$V$的一个基$\mathcal V=(v_1,\dots,v_n)$，每个$v\in V$都可以*唯一地*表示成
$$v=a_1v_1+\dots+a_nv_n$$

那么在一定程度上，$v$和数组$(a_1,\dots,a_n)$就是等价的。我们就把后者称为$v$在$\mathcal V$上的**坐标**。明确地写，就是：
$$
v=\begin{bmatrix}
a_1 \\
\vdots \\
a_n
\end{bmatrix}_{\mathcal V}
$$

现在我们回到线性映射上来，我们之前提到，线性映射由它对基向量的作用决定，也就是由$Tv_1,\dots,Tv_n$决定，这里我们选出一个代表$Tv_k$来研究。

由于$Tv_k\in W$，而$W$也有一个基$\mathcal W=(w_1,\dots,w_m)$，因此可以写成
$$
Tv_k=A_{1k}w_1+\dots+A_{mk}w_m=
\begin{bmatrix}
A_{1k} \\
\vdots \\
A_{mk}
\end{bmatrix}_{\mathcal W}
$$

注意看这里的系数下标是如何对应的。

现在我们已经走到了最后一步：由于每个线性映射都由$n$个$Tv_k$决定，而每个$Tv_k$都由$m$个数$A_{jk}$决定，所以总的来说，每个线性映射都由这$m\times n$个数完全决定！于是我们就把这$m\times n$个数放在一起，称为线性映射$T$在$\mathcal V,\mathcal W$上的**矩阵**
$$
A=(A_{jk})_{m\times n}=
\begin{bmatrix}
A_{11} & \dots & A_{1n}\\
\vdots & \ddots & \vdots \\
A_{m1} & \dots & A_{mn}
\end{bmatrix}
$$
用$\mathcal M(T,\mathcal V,\mathcal W)$或$\mathcal M(T)$表示。

从矩阵的定义中，我们也能看出应该如何定义矩阵与向量的乘法：矩阵$A$与向量$v$的乘积只是$A$的列的线性组合，因为$A$乘以基向量就只是提取出$A$的一列，如：
$$
\begin{bmatrix}
A_{11} & \dots & A_{1n}\\
\vdots & \ddots & \vdots \\
A_{m1} & \dots & A_{mn}
\end{bmatrix}
\begin{bmatrix}
1 \\
0 \\
\vdots \\
0
\end{bmatrix}=\begin{bmatrix}
A_{11} \\
\vdots \\
A_{m1}
\end{bmatrix}
$$

因此，如果我们记$(a_1,\dots,a_n)$为$\mathcal M(v,\mathcal V)$，就有
$$\mathcal M(Tv)=\mathcal M(T)\mathcal M(v)$$

同样，我们也可以根据线性映射的加法和数乘定义矩阵的加法和数乘：
$$
\mathcal M(S+T)=\mathcal M(S)+\mathcal M(T),
\mathcal M(\lambda T)=\lambda \mathcal M(T)
$$

现在我们要定义矩阵的乘法，仿照上面的例子，我们很自然地要求
$$\mathcal M(ST)=\mathcal M(S)\mathcal M(T)$$

设$T\colon U\to V,S\colon V\to W,\mathcal M(S)=A,\mathcal M(T)=B$并且$U,V,W$的维数分别为$p,m,n$，那么
$$
STu_k=S\sum_{r=1}^{m} B_{rk}v_r
=\sum_{r=1}^{m} B_{rk}Sv_r
=\sum_{j=1}^{n}\sum_{r=1}^{m} A_{jr}B_{rk} w_j
$$

根据矩阵的定义，我们知道$\mathcal M(ST)$的$(j,k)$位置的元素为$\sum_{r=1}^{m} A_{jr}B_{rk}$，这也正是矩阵乘法的一般定义：左乘矩阵的一行乘以右乘矩阵的一列。但是，在我们的这个定义下，矩阵乘法有更丰富的含义：矩阵乘法对应于线性映射的复合，因此线性映射的性质就自然地对应到了矩阵的性质。

例如，因为映射的复合是结合的：$(RS)T=R(ST)$，因此矩阵的乘法就是结合的：$(AB)C=A(BC)$。这样就避免了繁琐的计算，直达其本质。

# Section 4: 总结

本期，我们以向量空间作为起点，逐步发现了向量空间中的结构：基与维数，并研究了向量空间之间的关联：线性映射，回答了“矩阵是什么”的问题：矩阵是线性映射在基上的表示。

下一次，我们将研究与每个线性映射相关的子空间：线性映射的**核**与**像**，以及它们是如何与线性方程组关联起来的。我们还将尝试在不使用行变换的情况下证明线性方程组中的一些重要结论。

本期内容就是这样，感谢观看！